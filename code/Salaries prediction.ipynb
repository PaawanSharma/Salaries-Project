{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary Predictions Based on Job Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 1 - DEFINE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Define the problem ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project intends to predict salaries for given job descriptions.\n",
    "\n",
    "Job descriptions have eight __features__:\n",
    "\n",
    "* __jobId__ = a unique index for each job\n",
    "\n",
    "* __companyId__ = a categorical ID for the company the job is for\n",
    "\n",
    "* __jobType__ = a categorical feature describing the role\n",
    "\n",
    "* __degree__ = a categorical feature describing the required education level\n",
    "\n",
    "* __major__ = a categorical feature conveying the field in which a degree is required, if any\n",
    "\n",
    "* __industry__ = a categorical feature describing the industry to which the job belongs\n",
    "\n",
    "* __yearsExperience__ = a numerical feature measuring how many years of work experience are required for the role\n",
    "\n",
    "* __milesFromMetropolis__ = a numerical feature measuring how far the workplace is located from a metropolis\n",
    "\n",
    "\n",
    "The __target__ is __salary__ (in 1000 USD). Salaries are given in the training set and need to be predicted for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Local imports\n",
    "from eda.stats import interquartile_rule\n",
    "# from eda import plot\n",
    "from feature_engineering import encoders\n",
    "from exceptions import NotUniqueError\n",
    "from model_selection import Log, Test_Combination\n",
    "# import preprocessing\n",
    "\n",
    "# Author information\n",
    "__author__ = \"Paawan Sharma\"\n",
    "__email__ = \"paawansharma@protonmail.com\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "LOGPATH = \"../models_log.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prevents jupyter from creating scrollable subframes for plots, instead showing the entire set of generated plots without the need for scrolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 2 - DISCOVER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Load the data ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data in pandas DataFrames\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train_features.csv\", header=0)\n",
    "train_data[\"salary\"] = pd.read_csv(\"../data/train_salaries.csv\", header=0)[\"salary\"]\n",
    "\n",
    "test_data = pd.read_csv(\"../data/test_features.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take an initial look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(train_data)\n",
    "# print(\"=\" * 100)\n",
    "# train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(test_data)\n",
    "# print(\"=\" * 100)\n",
    "# test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Clean the data ----\n",
    "\n",
    "### Look for duplicate data in the training set. Duplicate job IDs may indicate corrupt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"There are {} duplicate rows in the training set.\".format(\n",
    "#         train_data.duplicated().sum()\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"There are {} duplicate jobIDs in the training set.\".format(\n",
    "#         train_data[\"jobId\"].duplicated().sum()\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for invalid data.\n",
    "\n",
    "We know from earlier that there are no null values. We can check that all values are appropriate for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numerical features in both dataframes\n",
    "\n",
    "# for df_name, df in {\"test set\": test_data, \"training set\": train_data}.items():\n",
    "#     print(\"Checking {}\".format(df_name))\n",
    "#     print(\"Are all years of experience non-negative?\")\n",
    "#     print(df[df[\"yearsExperience\"] < 0].empty)\n",
    "#     print(\"Are all miles from metropolis non-negative?\")\n",
    "#     print(df[df[\"milesFromMetropolis\"] < 0].empty)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# print(\"Are all salary values in training set positive?\")\n",
    "# print(train_data[train_data[\"salary\"] <= 0].empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe jobs whose salaries are not positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[train_data[\"salary\"] <= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these salaries is negative. A salary can be zero (or rounded down to zero) if the employee opts to receive alternative compensation so these data may be valid. However, it is also possible these are missing values. Since the training dataset is large (1,000,000 samples), it is safe to drop these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is now 999,995.\n"
     ]
    }
   ],
   "source": [
    "train_data.drop(train_data[train_data[\"salary\"] == 0].index, inplace=True)\n",
    "print(\"The number of samples is now {:,}.\".format(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical features in both dataframes\n",
    "\n",
    "# for df_name, df in {\"training set\": train_data, \"test set\": test_data}.items():\n",
    "#     print(\"Checking {}\\n\".format(df_name))\n",
    "#     for feature in [\"jobType\", \"degree\", \"major\", \"industry\"]:\n",
    "#         print(\"Values for {} are: {}\\n\".format(feature, list(df[feature].unique())))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values appear to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that majors are only given for jobs that require degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_data[train_data[\"major\"] != \"NONE\"].degree.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As jobIDs are unique, we can set them as the indices for the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_data, test_data]:\n",
    "    df.set_index(\"jobId\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, check that none of the training jobIds are repeated in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"The intersection contains {} samples.\".format(\n",
    "#         train_data.index.intersection(test_data.index).size\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Explore the data (EDA) ----\n",
    "\n",
    "### Start by getting a description of all categorical and numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.describe(include=[\"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that there are 63 different companies in the dataset. The other categorical features have few unique values and therefore will probably be more useful to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All numerical ranges seem reasonable.\n",
    "\n",
    "### Explore the distribution of the target (salary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_theme(style=\"whitegrid\", palette=\"colorblind\")\n",
    "# sns.set_context(\"poster\")\n",
    "\n",
    "# plot.plot_target(\"salary\", train_data, target_label=\"Salary / 1000 USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The salary is approximately normally distributed. There are some outliers on the upper end of the distribution. These can be explored using the interquartile rule for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary_outliers, salary_uppers, salary_lowers = interquartile_rule(\"salary\", train_data)\n",
    "\n",
    "# print(\"There are {} lower outliers.\".format(salary_lowers))\n",
    "\n",
    "# for feature in [\"jobType\", \"degree\", \"industry\"]:\n",
    "#     display(salary_outliers[feature].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the outliers, which are all upper outliers, correspond to typically high-paying roles, high educational qualifications and profitable industries (such as oil and finance). This is to be expected and provides confidence in the validity of the data.\n",
    "\n",
    "It is worthwhile to explore those outliers which require no educational qualification. There are 208 such outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary_outliers[salary_outliers.degree == \"NONE\"].jobType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all typically high-paying roles, which is a good indication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate counts and correlations with salary for each categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categoricals = list(train_data.columns[train_data.dtypes == \"object\"])\n",
    "# numericals = list(train_data.columns[train_data.dtypes == \"int64\"])[:-1]\n",
    "\n",
    "# for feature in categoricals:\n",
    "#     plot.plot_categorical(feature, \"salary\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__companyId__ has no association with salary. It is worth investigating whether the number of dataset samples belonging to a given company has any relationship with salary as this might convey some information about the size of the company and hence perhaps the salaries it pays.\n",
    "\n",
    "__jobType__ shows a clear association with salary. Chief roles are the highest paying and janitor the lowest.\n",
    "\n",
    "__degree__ shows a clear difference in salaries between those without a degree and those with one. Beyond this, the correlation is slight.\n",
    "\n",
    "__major__ shows a very slight association, with engineering being the major corresponding to the highest median salary.\n",
    "\n",
    "__industry__ shows a clear association, with oil and finance having the highest median salaries.\n",
    "\n",
    "All correlations appear to be linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot.categorical_correlation(\n",
    "#     feature=\"companyId\",\n",
    "#     target=\"salary\",\n",
    "#     dataframe=train_data,\n",
    "#     groupfunc=np.size,\n",
    "#     x_label=\"Number of entries for company in training data\",\n",
    "#     y_label=\"Mean salary / 1000 USD\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a _very_ slight positive correlation between the number of jobs the company has in the training data and the mean salary for jobs in that company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate associations of numerical features with salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for feature in numericals:\n",
    "#     plot.plot_numerical(feature, \"salary\", train_data, target_unit=\"1000 USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both numerical featuress' counts have approximately uniform distributions.\n",
    "\n",
    "__yearsExperience__ is positively correlated with salary. This is to be expected, since career progression over time usually results in higher pay.\n",
    "\n",
    "__milesFromMetropolis__ is negatively correlated with salary. Again, this is to be expected: jobs in cities tend to offer higher salaries to account for the generally higher costs of living."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for correlations between features\n",
    "\n",
    "Categorical features need to be encoded before the relationships they have with other features and the target can be explored.\n",
    "\n",
    "There are multiple options for encoding:\n",
    "\n",
    "1. __dummy coding__: one-hot encoding with one dummy variable dropped\n",
    "\n",
    "2. __ordinal encoding on ordered categories__: order categories by some metric (such as mean salary) then encode with integers\n",
    " * Metrics to consider: mean, median\n",
    "\n",
    "3. __label encoding__: encode categories with some metric (such as mean salary)\n",
    " * Metrics to consider: mean, median\n",
    " \n",
    "Although each of these encodings (particularly the latter two) will convey similar information about the dataset, it is worthwhile to see the heatmaps they each produce as we shall need to use encoding later on for feature engineering. Therefore, we shall explore each of these encodings in turn here.\n",
    "\n",
    "#### Dummy coding\n",
    "\n",
    "For dummy coding, we will exclude companyId from our heatmap as it has a very weak association with salary and because the large number of dummy variables would render the heatmap intractable.\n",
    "\n",
    "Also note that one dummy variable has been dropped from each category so as not to introduce collinearity. Therefore, for instance, the heatmap will not contain a column or row for jobType=CEO (the first level in the alphabetically sorted column of jobTypes).\n",
    "\n",
    "A clustermap was generated in order to get a new index ordering that would more clearly display the clusters of correlated variables. The new order is passed to the correlation_map method of our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vlag = sns.color_palette(\"vlag\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reordering = [\n",
    "#     11,\n",
    "#     5,\n",
    "#     13,\n",
    "#     20,\n",
    "#     22,\n",
    "#     1,\n",
    "#     6,\n",
    "#     8,\n",
    "#     26,\n",
    "#     27,\n",
    "#     7,\n",
    "#     24,\n",
    "#     25,\n",
    "#     3,\n",
    "#     23,\n",
    "#     4,\n",
    "#     0,\n",
    "#     2,\n",
    "#     9,\n",
    "#     18,\n",
    "#     17,\n",
    "#     16,\n",
    "#     12,\n",
    "#     21,\n",
    "#     15,\n",
    "#     19,\n",
    "#     10,\n",
    "#     14,\n",
    "# ]\n",
    "\n",
    "# dummy_encoder = encoders.Dummy_Encoder(exclude=[\"companyId\"])\n",
    "# dummy_encoder.fit(train_data)\n",
    "# dummy_encoder.correlation_map(train_data, vlag, 175, 7.5, 1, reordering=reordering)\n",
    "# del dummy_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations of interest__\n",
    "\n",
    "\n",
    "* __Salary__ is __positively correlated__ with\n",
    "\n",
    " * yearsExperience (the most positive correlation),\n",
    " \n",
    " * degree_DOCTORAL, degree_MASTERS,\n",
    " \n",
    " * jobType_CFO, jobType_CTO, jobtype_VICE_PRESIDENT\n",
    " \n",
    " * industry_OIL, industry_FINANCE, industry_WEB,\n",
    " \n",
    " * and all dummy variables indicating an existent major, with major_ENGINEERING having the highest correlation and major_LITERATURE the lowest.\n",
    " \n",
    "* __Salary__ is __negatively correlated__ with\n",
    "\n",
    " * jobType_JANITOR (the highest magnitude correlation), jobType_JUNIOR, jobType_MANAGER\n",
    " ,\n",
    " * major_NONE,\n",
    " \n",
    " * milesFromMetropolis,\n",
    " \n",
    " * degree_NONE, degree_HIGH_SCHOOL,\n",
    " \n",
    " * and industry_EDUCATION, industry_SERVICE, industry_HEALTH.\n",
    "\n",
    "* major_NONE is strongly positively correlated with degree_HIGH_SCHOOL and degree_NONE (as noted earlier) and with jobType_JANITOR.\n",
    "\n",
    "* As expected, degree_HIGH_SCHOOL and degree_NONE are positively correlated with jobType_JANITOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal encoding on ordered categories\n",
    "\n",
    "For ordinal encoding, we will consider ordering both by mean salary and by median salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for average in [np.mean, np.median]:\n",
    "#     print(\"\\n\".join([average.__name__.title(), \"=\" * len(average.__name__)]))\n",
    "#     try:\n",
    "#         ordinal_encoder = encoders.Ordinal_Encoder(average, \"salary\")\n",
    "#         ordinal_encoder.fit(train_data)\n",
    "#         ordinal_encoder.correlation_map(train_data, vlag, 100, 10, 2)\n",
    "#         del ordinal_encoder\n",
    "#     except NotUniqueError as nue:\n",
    "#         print(nue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations__\n",
    "\n",
    "* Ordinal encoding is much easier to interpret than dummy coding as it doesn't produce a large array.\n",
    "* We have already seen the associations between salary and individual features but we can now quantify these associations.\n",
    "* milesFromMetropolis is the only feature negatively correlated with salary (note that categorical features cannot have negative associations with the target due to the way values are ordered for ordinal encoding).\n",
    "* companyId has a very weak association with salary.\n",
    "* degree and yearsExperience have the highest postive correlations with salary, followed by major and industry in that order.\n",
    "* We can see that degree, major and jobType are positively associated with each other under ordinal encoding. The relationship between degree and major is particularly strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoding\n",
    "\n",
    "For target encoding, we shall only be using the mean for labelling groups as the median will result in the same error as in ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_encoder = encoders.Target_Encoder(np.mean, \"salary\")\n",
    "# target_encoder.fit(train_data)\n",
    "# target_encoder.correlation_map(train_data, vlag, 100, 10, 2)\n",
    "# del target_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations__\n",
    "\n",
    "The results are similar to those for ordinal encoding but magnitudes of correlations are generally slightly greater. This is to be expected as target encoding preserves more information about the magnitudes of mean salary values of groups than a mere ordering does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Establish a baseline ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metric\n",
    "\n",
    "We shall use mean squared error (MSE) as a metric for assessing the accuracy of models.\n",
    "\n",
    "### Baseline model\n",
    "\n",
    "Our baseline model will be mean salary for each job type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_salaries = train_data[\"salary\"]\n",
    "\n",
    "# # Calculate baseline predictions\n",
    "# mapping = train_data.groupby(\"jobType\")[\"salary\"].mean()\n",
    "# baseline_pred = train_data[\"jobType\"].replace(mapping)\n",
    "\n",
    "# # MSE\n",
    "# baseline_mse = mean_squared_error(true_salaries, baseline_pred)\n",
    "# print(\"The MSE of the baseline model is {}\".format(baseline_mse))\n",
    "\n",
    "# del true_salaries\n",
    "# del mapping\n",
    "# del baseline_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Hypothesise a solution -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to try\n",
    "\n",
    "Kinds of model we could try for this problem are:\n",
    "\n",
    "* multiple linear regression\n",
    "\n",
    "* support vector regression\n",
    "\n",
    "* decision tree regression\n",
    "\n",
    "* random forest regression\n",
    "\n",
    "* gradient boosting regression\n",
    "\n",
    "\n",
    "### Feature selection and engineering\n",
    "\n",
    "\n",
    "* companyId is not associated with salary under any of the encoding schemes we tried, hence we could try dropping it from the data.\n",
    "\n",
    "* Target encoding would be a good encoding system to try as it doesn't produce a high number of features (and so doesn't require us to drop companyId) and gives good correlations with salary.\n",
    "\n",
    "* For linear models, we need to scale our features.\n",
    "\n",
    "* We could engineer pairwise interaction features. These may help with linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 3 - DEVELOP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Engineer features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_encoder = encoders.Dummy_Encoder(exclude=[\"companyId\"])\n",
    "# target_encoder = encoders.Target_Encoder(np.mean, \"salary\")\n",
    "# ordinal_encoder = encoders.Ordinal_Encoder(np.mean, \"salary\")\n",
    "# pairwise = PolynomialFeatures(2, interaction_only=True, include_bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Create models -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_regression = Test_Combination(\n",
    "#     encoder=encoders.Dummy_Encoder(exclude=[\"companyId\"]), regressor=LinearRegression()\n",
    "# )\n",
    "\n",
    "\n",
    "# linear_regression_pw = Test_Combination(\n",
    "#     encoder=encoders.Target_Encoder(np.mean, \"salary\"),\n",
    "#     interactions=PolynomialFeatures(\n",
    "#         degree=2, interaction_only=True, include_bias=False\n",
    "#     ),\n",
    "#     regressor=LinearRegression(),\n",
    "# )\n",
    "\n",
    "\n",
    "# linear_SVR_t = Test_Combination(\n",
    "#     encoder=encoders.Target_Encoder(np.mean, \"salary\"),\n",
    "#     scale=StandardScaler(),\n",
    "#     regressor=LinearSVR(),\n",
    "# )\n",
    "\n",
    "\n",
    "# linear_SVR_o = Test_Combination(\n",
    "#     encoder=encoders.Ordinal_Encoder(np.mean, \"salary\"),\n",
    "#     scale=StandardScaler(),\n",
    "#     regressor=LinearSVR(),\n",
    "# )\n",
    "\n",
    "\n",
    "# trees = [\n",
    "#     Test_Combination(\n",
    "#         encoder=encoders.Target_Encoder(np.mean, \"salary\"),\n",
    "#         regressor=DecisionTreeRegressor(max_depth=md),\n",
    "#     )\n",
    "#     for md in [None, 10, 15, 20, 25]\n",
    "# ]\n",
    "\n",
    "\n",
    "# trees = [\n",
    "#     Test_Combination(\n",
    "#         encoder=encoders.Target_Encoder(np.mean, \"salary\"),\n",
    "#         regressor=DecisionTreeRegressor(max_depth=md),\n",
    "#     )\n",
    "#     for md in range(11, 15)\n",
    "# ]\n",
    "\n",
    "# full_dataset_models = [\n",
    "#     linear_regression,\n",
    "#     linear_regression_pw,\n",
    "#     linear_SVR_t,\n",
    "#     linear_SVR_o,\n",
    "# ] + trees\n",
    "\n",
    "# full_dataset_models = trees\n",
    "\n",
    "# support_vector_t = Test_Combination(\n",
    "#     encoder=encoders.Target_Encoder(np.mean, \"salary\"), regressor=SVR()\n",
    "# )\n",
    "\n",
    "support_vector_o = Test_Combination(\n",
    "    encoder=encoders.Ordinal_Encoder(np.mean, \"salary\"), regressor=SVR()\n",
    ")\n",
    "\n",
    "\n",
    "subset_models = [support_vector_o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Test models -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results staged for logging!\n",
      "Mean MSE for DecisionTreeRegressor(max_depth=11) with Target_Encoder(metric=mean, target=salary, features=ALL): 396.68202005619946\n",
      "Results saved in log!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results staged for logging!\n",
      "Mean MSE for DecisionTreeRegressor(max_depth=12) with Target_Encoder(metric=mean, target=salary, features=ALL): 389.20814777965205\n",
      "Results saved in log!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   15.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results staged for logging!\n",
      "Mean MSE for DecisionTreeRegressor(max_depth=13) with Target_Encoder(metric=mean, target=salary, features=ALL): 387.81877113869024\n",
      "Results saved in log!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results staged for logging!\n",
      "Mean MSE for DecisionTreeRegressor(max_depth=14) with Target_Encoder(metric=mean, target=salary, features=ALL): 395.880962688231\n",
      "Results saved in log!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.8s finished\n"
     ]
    }
   ],
   "source": [
    "log = Log(source=LOGPATH)\n",
    "\n",
    "# for model in full_dataset_models:\n",
    "#     model.run(train_data, \"salary\", log)\n",
    "#     log.update_logfile(LOGPATH)\n",
    "\n",
    "for model in subset_models:\n",
    "    sample = train_data.sample(frac=0.1)\n",
    "    model.run(sample, \"salary\", log)\n",
    "    log.update_logfile(LOGPATH)\n",
    "\n",
    "# sample = train_data.sample(frac=0.1)\n",
    "# support_vector_o.run(sample, \"salary\", log)\n",
    "# log.update_logfile(LOGPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Regressor</th>\n",
       "      <th>CSV_1_MSE</th>\n",
       "      <th>CSV_2_MSE</th>\n",
       "      <th>CSV_3_MSE</th>\n",
       "      <th>CSV_4_MSE</th>\n",
       "      <th>CSV_5_MSE</th>\n",
       "      <th>mean MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>376.618461</td>\n",
       "      <td>375.466403</td>\n",
       "      <td>376.601309</td>\n",
       "      <td>372.737659</td>\n",
       "      <td>372.609375</td>\n",
       "      <td>374.806641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>377.355884</td>\n",
       "      <td>374.522295</td>\n",
       "      <td>383.996385</td>\n",
       "      <td>379.417059</td>\n",
       "      <td>372.266337</td>\n",
       "      <td>377.511592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>386.309129</td>\n",
       "      <td>385.191988</td>\n",
       "      <td>385.992473</td>\n",
       "      <td>382.570777</td>\n",
       "      <td>382.007319</td>\n",
       "      <td>384.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=13)</td>\n",
       "      <td>389.087208</td>\n",
       "      <td>389.387598</td>\n",
       "      <td>389.108787</td>\n",
       "      <td>385.666604</td>\n",
       "      <td>385.759048</td>\n",
       "      <td>387.801849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=13)</td>\n",
       "      <td>389.115095</td>\n",
       "      <td>389.369964</td>\n",
       "      <td>389.105547</td>\n",
       "      <td>385.715658</td>\n",
       "      <td>385.787591</td>\n",
       "      <td>387.818771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=12)</td>\n",
       "      <td>391.197449</td>\n",
       "      <td>391.063225</td>\n",
       "      <td>390.024004</td>\n",
       "      <td>386.611105</td>\n",
       "      <td>387.141388</td>\n",
       "      <td>389.207434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=12)</td>\n",
       "      <td>391.205546</td>\n",
       "      <td>391.066302</td>\n",
       "      <td>390.024004</td>\n",
       "      <td>386.608742</td>\n",
       "      <td>387.136145</td>\n",
       "      <td>389.208148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=14)</td>\n",
       "      <td>397.531487</td>\n",
       "      <td>397.307928</td>\n",
       "      <td>396.883395</td>\n",
       "      <td>393.907554</td>\n",
       "      <td>393.618997</td>\n",
       "      <td>395.849872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=14)</td>\n",
       "      <td>397.578963</td>\n",
       "      <td>397.380671</td>\n",
       "      <td>396.856623</td>\n",
       "      <td>394.035558</td>\n",
       "      <td>393.552999</td>\n",
       "      <td>395.880963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=11)</td>\n",
       "      <td>398.580761</td>\n",
       "      <td>398.506735</td>\n",
       "      <td>397.701574</td>\n",
       "      <td>394.044318</td>\n",
       "      <td>394.576712</td>\n",
       "      <td>396.682020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=11)</td>\n",
       "      <td>398.580761</td>\n",
       "      <td>398.506735</td>\n",
       "      <td>397.701574</td>\n",
       "      <td>394.047195</td>\n",
       "      <td>394.576712</td>\n",
       "      <td>396.682595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>LinearSVR()</td>\n",
       "      <td>399.953039</td>\n",
       "      <td>398.966319</td>\n",
       "      <td>400.485412</td>\n",
       "      <td>396.185386</td>\n",
       "      <td>395.723205</td>\n",
       "      <td>398.262672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T</td>\n",
       "      <td>LinearSVR()</td>\n",
       "      <td>402.565014</td>\n",
       "      <td>401.704362</td>\n",
       "      <td>403.167805</td>\n",
       "      <td>398.942643</td>\n",
       "      <td>398.281108</td>\n",
       "      <td>400.932186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=10)</td>\n",
       "      <td>410.365714</td>\n",
       "      <td>409.109294</td>\n",
       "      <td>409.756633</td>\n",
       "      <td>405.617084</td>\n",
       "      <td>406.602948</td>\n",
       "      <td>408.290334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=10)</td>\n",
       "      <td>410.365714</td>\n",
       "      <td>409.109294</td>\n",
       "      <td>409.756633</td>\n",
       "      <td>405.617084</td>\n",
       "      <td>406.602948</td>\n",
       "      <td>408.290334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=15)</td>\n",
       "      <td>415.669361</td>\n",
       "      <td>415.637571</td>\n",
       "      <td>415.800191</td>\n",
       "      <td>412.567670</td>\n",
       "      <td>412.562367</td>\n",
       "      <td>414.447432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=15)</td>\n",
       "      <td>415.746882</td>\n",
       "      <td>415.388172</td>\n",
       "      <td>415.888367</td>\n",
       "      <td>413.022655</td>\n",
       "      <td>412.458817</td>\n",
       "      <td>414.500979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=9)</td>\n",
       "      <td>425.918689</td>\n",
       "      <td>426.004687</td>\n",
       "      <td>427.064035</td>\n",
       "      <td>421.772843</td>\n",
       "      <td>422.810710</td>\n",
       "      <td>424.714193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=8)</td>\n",
       "      <td>448.846449</td>\n",
       "      <td>448.546507</td>\n",
       "      <td>449.939010</td>\n",
       "      <td>444.371129</td>\n",
       "      <td>445.044864</td>\n",
       "      <td>447.349592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=7)</td>\n",
       "      <td>480.763231</td>\n",
       "      <td>479.048669</td>\n",
       "      <td>481.271477</td>\n",
       "      <td>474.629281</td>\n",
       "      <td>474.803187</td>\n",
       "      <td>478.103169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=6)</td>\n",
       "      <td>527.410989</td>\n",
       "      <td>525.447438</td>\n",
       "      <td>528.748553</td>\n",
       "      <td>522.584963</td>\n",
       "      <td>520.331128</td>\n",
       "      <td>524.904614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=5)</td>\n",
       "      <td>588.569441</td>\n",
       "      <td>586.030462</td>\n",
       "      <td>588.399047</td>\n",
       "      <td>584.115214</td>\n",
       "      <td>582.169000</td>\n",
       "      <td>585.856633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=20)</td>\n",
       "      <td>621.758717</td>\n",
       "      <td>621.834963</td>\n",
       "      <td>622.306447</td>\n",
       "      <td>621.446389</td>\n",
       "      <td>618.790691</td>\n",
       "      <td>621.227441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=20)</td>\n",
       "      <td>622.570398</td>\n",
       "      <td>621.585416</td>\n",
       "      <td>623.578503</td>\n",
       "      <td>621.191928</td>\n",
       "      <td>619.134068</td>\n",
       "      <td>621.612063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=25)</td>\n",
       "      <td>737.333941</td>\n",
       "      <td>735.892633</td>\n",
       "      <td>738.490536</td>\n",
       "      <td>738.001918</td>\n",
       "      <td>730.234659</td>\n",
       "      <td>735.990738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=25)</td>\n",
       "      <td>737.073659</td>\n",
       "      <td>736.225734</td>\n",
       "      <td>739.527526</td>\n",
       "      <td>737.985882</td>\n",
       "      <td>732.114313</td>\n",
       "      <td>736.585423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=50)</td>\n",
       "      <td>750.650839</td>\n",
       "      <td>748.057617</td>\n",
       "      <td>751.643952</td>\n",
       "      <td>751.247768</td>\n",
       "      <td>745.976191</td>\n",
       "      <td>749.515274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor()</td>\n",
       "      <td>751.843471</td>\n",
       "      <td>748.995167</td>\n",
       "      <td>751.250439</td>\n",
       "      <td>750.681567</td>\n",
       "      <td>745.872065</td>\n",
       "      <td>749.728542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=70)</td>\n",
       "      <td>751.503620</td>\n",
       "      <td>748.459518</td>\n",
       "      <td>750.224567</td>\n",
       "      <td>752.931963</td>\n",
       "      <td>746.003476</td>\n",
       "      <td>749.824629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T</td>\n",
       "      <td>DecisionTreeRegressor()</td>\n",
       "      <td>750.759369</td>\n",
       "      <td>748.845302</td>\n",
       "      <td>751.125851</td>\n",
       "      <td>751.869207</td>\n",
       "      <td>747.307836</td>\n",
       "      <td>749.981513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=40)</td>\n",
       "      <td>752.783802</td>\n",
       "      <td>749.097327</td>\n",
       "      <td>749.613073</td>\n",
       "      <td>752.674644</td>\n",
       "      <td>746.958857</td>\n",
       "      <td>750.225541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>O</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=60)</td>\n",
       "      <td>752.724777</td>\n",
       "      <td>750.154856</td>\n",
       "      <td>750.033424</td>\n",
       "      <td>752.294078</td>\n",
       "      <td>746.231037</td>\n",
       "      <td>750.287634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Encoder                            Regressor   CSV_1_MSE   CSV_2_MSE  \\\n",
       "1        T                   LinearRegression()  376.618461  375.466403   \n",
       "9        T                                SVR()  377.355884  374.522295   \n",
       "0        D                   LinearRegression()  386.309129  385.191988   \n",
       "26       O  DecisionTreeRegressor(max_depth=13)  389.087208  389.387598   \n",
       "30       T  DecisionTreeRegressor(max_depth=13)  389.115095  389.369964   \n",
       "25       O  DecisionTreeRegressor(max_depth=12)  391.197449  391.063225   \n",
       "29       T  DecisionTreeRegressor(max_depth=12)  391.205546  391.066302   \n",
       "27       O  DecisionTreeRegressor(max_depth=14)  397.531487  397.307928   \n",
       "31       T  DecisionTreeRegressor(max_depth=14)  397.578963  397.380671   \n",
       "28       T  DecisionTreeRegressor(max_depth=11)  398.580761  398.506735   \n",
       "24       O  DecisionTreeRegressor(max_depth=11)  398.580761  398.506735   \n",
       "3        O                          LinearSVR()  399.953039  398.966319   \n",
       "2        T                          LinearSVR()  402.565014  401.704362   \n",
       "11       O  DecisionTreeRegressor(max_depth=10)  410.365714  409.109294   \n",
       "5        T  DecisionTreeRegressor(max_depth=10)  410.365714  409.109294   \n",
       "12       O  DecisionTreeRegressor(max_depth=15)  415.669361  415.637571   \n",
       "6        T  DecisionTreeRegressor(max_depth=15)  415.746882  415.388172   \n",
       "23       O   DecisionTreeRegressor(max_depth=9)  425.918689  426.004687   \n",
       "22       O   DecisionTreeRegressor(max_depth=8)  448.846449  448.546507   \n",
       "21       O   DecisionTreeRegressor(max_depth=7)  480.763231  479.048669   \n",
       "20       O   DecisionTreeRegressor(max_depth=6)  527.410989  525.447438   \n",
       "19       O   DecisionTreeRegressor(max_depth=5)  588.569441  586.030462   \n",
       "13       O  DecisionTreeRegressor(max_depth=20)  621.758717  621.834963   \n",
       "7        T  DecisionTreeRegressor(max_depth=20)  622.570398  621.585416   \n",
       "14       O  DecisionTreeRegressor(max_depth=25)  737.333941  735.892633   \n",
       "8        T  DecisionTreeRegressor(max_depth=25)  737.073659  736.225734   \n",
       "16       O  DecisionTreeRegressor(max_depth=50)  750.650839  748.057617   \n",
       "10       O              DecisionTreeRegressor()  751.843471  748.995167   \n",
       "18       O  DecisionTreeRegressor(max_depth=70)  751.503620  748.459518   \n",
       "4        T              DecisionTreeRegressor()  750.759369  748.845302   \n",
       "15       O  DecisionTreeRegressor(max_depth=40)  752.783802  749.097327   \n",
       "17       O  DecisionTreeRegressor(max_depth=60)  752.724777  750.154856   \n",
       "\n",
       "     CSV_3_MSE   CSV_4_MSE   CSV_5_MSE    mean MSE  \n",
       "1   376.601309  372.737659  372.609375  374.806641  \n",
       "9   383.996385  379.417059  372.266337  377.511592  \n",
       "0   385.992473  382.570777  382.007319  384.414337  \n",
       "26  389.108787  385.666604  385.759048  387.801849  \n",
       "30  389.105547  385.715658  385.787591  387.818771  \n",
       "25  390.024004  386.611105  387.141388  389.207434  \n",
       "29  390.024004  386.608742  387.136145  389.208148  \n",
       "27  396.883395  393.907554  393.618997  395.849872  \n",
       "31  396.856623  394.035558  393.552999  395.880963  \n",
       "28  397.701574  394.044318  394.576712  396.682020  \n",
       "24  397.701574  394.047195  394.576712  396.682595  \n",
       "3   400.485412  396.185386  395.723205  398.262672  \n",
       "2   403.167805  398.942643  398.281108  400.932186  \n",
       "11  409.756633  405.617084  406.602948  408.290334  \n",
       "5   409.756633  405.617084  406.602948  408.290334  \n",
       "12  415.800191  412.567670  412.562367  414.447432  \n",
       "6   415.888367  413.022655  412.458817  414.500979  \n",
       "23  427.064035  421.772843  422.810710  424.714193  \n",
       "22  449.939010  444.371129  445.044864  447.349592  \n",
       "21  481.271477  474.629281  474.803187  478.103169  \n",
       "20  528.748553  522.584963  520.331128  524.904614  \n",
       "19  588.399047  584.115214  582.169000  585.856633  \n",
       "13  622.306447  621.446389  618.790691  621.227441  \n",
       "7   623.578503  621.191928  619.134068  621.612063  \n",
       "14  738.490536  738.001918  730.234659  735.990738  \n",
       "8   739.527526  737.985882  732.114313  736.585423  \n",
       "16  751.643952  751.247768  745.976191  749.515274  \n",
       "10  751.250439  750.681567  745.872065  749.728542  \n",
       "18  750.224567  752.931963  746.003476  749.824629  \n",
       "4   751.125851  751.869207  747.307836  749.981513  \n",
       "15  749.613073  752.674644  746.958857  750.225541  \n",
       "17  750.033424  752.294078  746.231037  750.287634  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log_df['mean MSE'] = log_df.iloc[:, 5:10].mean(axis=1)\n",
    "log_df.sort_values(by='mean MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training sample size</th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Interactions</th>\n",
       "      <th>Scaling</th>\n",
       "      <th>Regressor</th>\n",
       "      <th>CSV_1_MSE</th>\n",
       "      <th>CSV_2_MSE</th>\n",
       "      <th>CSV_3_MSE</th>\n",
       "      <th>CSV_4_MSE</th>\n",
       "      <th>CSV_5_MSE</th>\n",
       "      <th>Time /s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100000</td>\n",
       "      <td>Target_Encoder(metric=mean, target=salary, fea...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>SVR()</td>\n",
       "      <td>-377.355884</td>\n",
       "      <td>-374.522295</td>\n",
       "      <td>-383.996385</td>\n",
       "      <td>-379.417059</td>\n",
       "      <td>-372.266337</td>\n",
       "      <td>2451.722201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Training sample size                                            Encoder  \\\n",
       "9                100000  Target_Encoder(metric=mean, target=salary, fea...   \n",
       "\n",
       "  Interactions Scaling Regressor   CSV_1_MSE   CSV_2_MSE   CSV_3_MSE  \\\n",
       "9         None    None     SVR() -377.355884 -374.522295 -383.996385   \n",
       "\n",
       "    CSV_4_MSE   CSV_5_MSE      Time /s  \n",
       "9 -379.417059 -372.266337  2451.722201  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.dataframe[log.dataframe['Regressor'] == 'SVR()']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Select best model -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 4 - DEPLOY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Automate pipeline -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Deploy solution -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Measure efficacy -----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
