{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary Predictions Based on Job Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 1 - DEFINE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Define the problem ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project intends to predict salaries for given job descriptions.\n",
    "\n",
    "Job descriptions have eight __features__:\n",
    "\n",
    "* __jobId__ = a unique index for each job\n",
    "\n",
    "* __companyId__ = a categorical ID for the company the job is for\n",
    "\n",
    "* __jobType__ = a categorical feature describing the role\n",
    "\n",
    "* __degree__ = a categorical feature describing the required education level\n",
    "\n",
    "* __major__ = a categorical feature conveying the field in which a degree is required, if any\n",
    "\n",
    "* __industry__ = a categorical feature describing the industry to which the job belongs\n",
    "\n",
    "* __yearsExperience__ = a numerical feature measuring how many years of work experience are required for the role\n",
    "\n",
    "* __milesFromMetropolis__ = a numerical feature measuring how far the workplace is located from a metropolis\n",
    "\n",
    "\n",
    "The __target__ is __salary__ (in 1000 USD). Salaries are given in the training set and need to be predicted for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Local imports\n",
    "from eda.stats import interquartile_rule\n",
    "from eda import plot\n",
    "from feature_engineering import encoders\n",
    "from exceptions import NotUniqueError\n",
    "\n",
    "# Author information\n",
    "__author__ = \"Paawan Sharma\"\n",
    "__email__ = \"paawansharma@protonmail.com\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell prevents jupyter from creating scrollable subframes for plots, instead showing the entire set of generated plots without the need for scrolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 2 - DISCOVER</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Load the data ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and test data in pandas DataFrames\n",
    "\n",
    "train_data = pd.read_csv(\"../data/train_features.csv\", header=0)\n",
    "train_data[\"salary\"] = pd.read_csv(\"../data/train_salaries.csv\", header=0)[\"salary\"]\n",
    "\n",
    "test_data = pd.read_csv(\"../data/test_features.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take an initial look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(train_data)\n",
    "# print(\"=\"*98)\n",
    "# train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(test_data)\n",
    "# print(\"=\"*88)\n",
    "# test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Clean the data ----\n",
    "\n",
    "### Look for duplicate data in the training set. Duplicate job IDs may indicate corrupt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"There are {} duplicate rows in the training set.\".format(\n",
    "#         train_data.duplicated().sum()\n",
    "#     )\n",
    "# )\n",
    "# print(\n",
    "#     \"There are {} duplicate jobIDs in the training set.\".format(\n",
    "#         train_data[\"jobId\"].duplicated().sum()\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for invalid data.\n",
    "\n",
    "We know from earlier that there are no null values. We can check that all values are appropriate for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numerical features in both dataframes\n",
    "\n",
    "# for df_name, df in {\"test set\": test_data, \"training set\": train_data}.items():\n",
    "#     print(\"Checking {}\".format(df_name))\n",
    "#     print(\"Are all years of experience non-negative?\")\n",
    "#     print(df[df[\"yearsExperience\"] < 0].empty)\n",
    "#     print(\"Are all miles from metropolis non-negative?\")\n",
    "#     print(df[df[\"milesFromMetropolis\"] < 0].empty)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# print(\"Are all salary values in training set positive?\")\n",
    "# print(train_data[train_data[\"salary\"] <= 0].empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe jobs whose salaries are not positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[train_data[\"salary\"] <= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these salaries is negative. A salary can be zero (or rounded down to zero) if the employee opts to receive alternative compensation so these data may be valid. However, it is also possible these are missing values. Since the training dataset is large (1,000,000 samples), it is safe to drop these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples is now 999,995.\n"
     ]
    }
   ],
   "source": [
    "train_data.drop(train_data[train_data[\"salary\"] == 0].index, inplace=True)\n",
    "print(\"The number of samples is now {:,}.\".format(train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical features in both dataframes\n",
    "\n",
    "# for df_name, df in {\"training set\": train_data, \"test set\": test_data}.items():\n",
    "#     print(\"Checking {}\\n\".format(df_name))\n",
    "#     for feature in [\"jobType\", \"degree\", \"major\", \"industry\"]:\n",
    "#         print(\"Values for {} are: {}\\n\".format(feature, list(df[feature].unique())))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values appear to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that majors are only given for jobs that require degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_data[train_data[\"major\"] != \"NONE\"].degree.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As jobIDs are unique, we can set them as the indices for the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_data, test_data]:\n",
    "    df.set_index(\"jobId\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, check that none of the training jobIds are repeated in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\n",
    "#     \"The intersection contains {} samples.\".format(\n",
    "#         train_data.index.intersection(test_data.index).size\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Explore the data (EDA) ----\n",
    "\n",
    "### Start by getting a description of all categorical and numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.describe(include=[\"O\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that there are 63 different companies in the dataset. The other categorical features have few unique values and therefore will probably be more useful to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.describe(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All numerical ranges seem reasonable.\n",
    "\n",
    "### Explore the distribution of the target (salary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", palette=\"colorblind\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# plot.plot_target(\"salary\", train_data, target_label=\"Salary / 1000 USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The salary is approximately normally distributed. There are some outliers on the upper end of the distribution. These can be explored using the interquartile rule for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary_outliers, salary_uppers, salary_lowers = interquartile_rule(\"salary\", train_data)\n",
    "\n",
    "# print(\"There are {} lower outliers.\".format(salary_lowers))\n",
    "\n",
    "# for feature in [\"jobType\", \"degree\", \"industry\"]:\n",
    "#     display(salary_outliers[feature].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the outliers, which are all upper outliers, correspond to typically high-paying roles, high educational qualifications and profitable industries (such as oil and finance). This is to be expected and provides confidence in the validity of the data.\n",
    "\n",
    "It is worthwhile to explore those outliers which require no educational qualification. There are 208 such outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary_outliers[salary_outliers.degree == \"NONE\"].jobType.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all typically high-paying roles, which is a good indication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate counts and correlations with salary for each categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = list(train_data.columns[train_data.dtypes == \"object\"])\n",
    "numericals = list(train_data.columns[train_data.dtypes == \"int64\"])[:-1]\n",
    "\n",
    "# for feature in categoricals:\n",
    "#     plot.plot_categorical(feature, \"salary\", train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__companyId__ has no association with salary. It is worth investigating whether the number of dataset samples belonging to a given company has any relationship with salary as this might convey some information about the size of the company and hence perhaps the salaries it pays.\n",
    "\n",
    "__jobType__ shows a clear association with salary. Chief roles are the highest paying and janitor the lowest.\n",
    "\n",
    "__degree__ shows a clear difference in salaries between those without a degree and those with one. Beyond this, the correlation is slight.\n",
    "\n",
    "__major__ shows a very slight association, with engineering being the major corresponding to the highest median salary.\n",
    "\n",
    "__industry__ shows a clear association, with oil and finance having the highest median salaries.\n",
    "\n",
    "All correlations appear to be linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot.categorical_correlation(\n",
    "#     feature=\"companyId\",\n",
    "#     target=\"salary\",\n",
    "#     dataframe=train_data,\n",
    "#     groupfunc=np.size,\n",
    "#     x_label=\"Number of entries for company in training data\",\n",
    "#     y_label=\"Mean salary / 1000 USD\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a _very_ slight positive correlation between the number of jobs the company has in the training data and the mean salary for jobs in that company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate associations of numerical features with salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for feature in numericals:\n",
    "#     plot.plot_numerical(feature, \"salary\", train_data, target_unit=\"1000 USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both numerical featuress' counts have approximately uniform distributions.\n",
    "\n",
    "__yearsExperience__ is positively correlated with salary. This is to be expected, since career progression over time usually results in higher pay.\n",
    "\n",
    "__milesFromMetropolis__ is negatively correlated with salary. Again, this is to be expected: jobs in cities tend to offer higher salaries to account for the generally higher costs of living."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for correlations between features\n",
    "\n",
    "Categorical features need to be encoded before the relationships they have with other features and the target can be explored.\n",
    "\n",
    "There are multiple options for encoding:\n",
    "\n",
    "1. __dummy coding__: one-hot encoding with one dummy variable dropped\n",
    "\n",
    "2. __ordinal encoding on ordered categories__: order categories by some metric (such as mean salary) then encode with integers\n",
    " * Metrics to consider: mean, median\n",
    "\n",
    "3. __label encoding__: encode categories with some metric (such as mean salary)\n",
    " * Metrics to consider: mean, median\n",
    " \n",
    "Although each of these encodings (particularly the latter two) will convey similar information about the dataset, it is worthwhile to see the heatmaps they each produce as we shall need to use encoding later on for feature engineering. Therefore, we shall explore each of these encodings in turn here.\n",
    "\n",
    "#### Dummy coding\n",
    "\n",
    "For dummy coding, we will exclude companyId from our heatmap as it has a very weak association with salary and because the large number of dummy variables would render the heatmap intractable.\n",
    "\n",
    "Also note that one dummy variable has been dropped from each category so as not to introduce collinearity. Therefore, for instance, the heatmap will not contain a column or row for jobType=CEO (the first level in the alphabetically sorted column of jobTypes).\n",
    "\n",
    "A clustermap was generated in order to get a new index ordering that would more clearly display the clusters of correlated variables. The new order is passed to the correlation_map method of our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vlag = sns.color_palette(\"vlag\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reordering = [\n",
    "#     11,\n",
    "#     5,\n",
    "#     13,\n",
    "#     20,\n",
    "#     22,\n",
    "#     1,\n",
    "#     6,\n",
    "#     8,\n",
    "#     26,\n",
    "#     27,\n",
    "#     7,\n",
    "#     24,\n",
    "#     25,\n",
    "#     3,\n",
    "#     23,\n",
    "#     4,\n",
    "#     0,\n",
    "#     2,\n",
    "#     9,\n",
    "#     18,\n",
    "#     17,\n",
    "#     16,\n",
    "#     12,\n",
    "#     21,\n",
    "#     15,\n",
    "#     19,\n",
    "#     10,\n",
    "#     14,\n",
    "# ]\n",
    "\n",
    "# dummy_encoder = encoders.Dummy_Encoder(exclude=['companyId'])\n",
    "# dummy_encoder.fit(train_data)\n",
    "# dummy_encoder.correlation_map(train_data, vlag, 175, 7.5, 1, reordering=reordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations of interest__\n",
    "\n",
    "\n",
    "* __Salary__ is __positively correlated__ with\n",
    "\n",
    " * yearsExperience (the most positive correlation),\n",
    " \n",
    " * degree_DOCTORAL, degree_MASTERS,\n",
    " \n",
    " * jobType_CFO, jobType_CTO, jobtype_VICE_PRESIDENT\n",
    " \n",
    " * industry_OIL, industry_FINANCE, industry_WEB,\n",
    " \n",
    " * and all dummy variables indicating an existent major, with major_ENGINEERING having the highest correlation and major_LITERATURE the lowest.\n",
    " \n",
    "* __Salary__ is __negatively correlated__ with\n",
    "\n",
    " * jobType_JANITOR (the highest magnitude correlation), jobType_JUNIOR, jobType_MANAGER\n",
    " ,\n",
    " * major_NONE,\n",
    " \n",
    " * milesFromMetropolis,\n",
    " \n",
    " * degree_NONE, degree_HIGH_SCHOOL,\n",
    " \n",
    " * and industry_EDUCATION, industry_SERVICE, industry_HEALTH.\n",
    "\n",
    "* major_NONE is strongly positively correlated with degree_HIGH_SCHOOL and degree_NONE (as noted earlier) and with jobType_JANITOR.\n",
    "\n",
    "* As expected, degree_HIGH_SCHOOL and degree_NONE are positively correlated with jobType_JANITOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal encoding on ordered categories\n",
    "\n",
    "For ordinal encoding, we will consider ordering both by mean salary and by median salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for average in [np.mean, np.median]:\n",
    "#     print(\"\\n\".join([average.__name__.title(), \"=\" * len(average.__name__)]))\n",
    "#     try:\n",
    "#         ordinal_encoder = encoders.Ordinal_Encoder(average, 'salary')\n",
    "#         ordinal_encoder.fit(train_data)\n",
    "#         ordinal_encoder.correlation_map(train_data, vlag, 100, 10, 2)\n",
    "#     except NotUniqueError as nue:\n",
    "#         print(nue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations__\n",
    "\n",
    "* Ordinal encoding is much easier to interpret than dummy coding as it doesn't produce a large array.\n",
    "* We have already seen the associations between salary and individual features but we can now quantify these associations.\n",
    "* milesFromMetropolis is the only feature negatively correlated with salary (note that categorical features cannot have negative associations with the target due to the way values are ordered for ordinal encoding).\n",
    "* companyId has a very weak association with salary.\n",
    "* degree and yearsExperience have the highest postive correlations with salary, followed by major and industry in that order.\n",
    "* We can see that degree, major and jobType are positively associated with each other under ordinal encoding. The relationship between degree and major is particularly strong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target encoding\n",
    "\n",
    "For target encoding, we shall only be using the mean for labelling groups as the median will result in the same error as in ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_encoder = encoders.Target_Encoder(np.mean, 'salary')\n",
    "# target_encoder.fit(train_data)\n",
    "# target_encoder.correlation_map(train_data, vlag, 100, 10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations__\n",
    "\n",
    "The results are similar to those for ordinal encoding but magnitudes of correlations are generally slightly greater. This is to be expected as target encoding preserves more information about the magnitudes of mean salary values of groups than a mere ordering does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Establish a baseline ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metric\n",
    "\n",
    "We shall use mean squared error (MSE) as a metric for assessing the accuracy of models.\n",
    "\n",
    "### Baseline model\n",
    "\n",
    "Our baseline model will be mean salary for each job type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the baseline model is 963.9252996562975\n"
     ]
    }
   ],
   "source": [
    "true_salaries = train_data[\"salary\"]\n",
    "\n",
    "# Calculate baseline predictions\n",
    "mapping = train_data.groupby(\"jobType\")[\"salary\"].mean()\n",
    "baseline_pred = train_data[\"jobType\"].replace(mapping)\n",
    "\n",
    "# MSE\n",
    "baseline_mse = mean_squared_error(true_salaries, baseline_pred)\n",
    "print(\"The MSE of the baseline model is {}\".format(baseline_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Hypothesise a solution -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models to try\n",
    "\n",
    "Kinds of model we could try for this problem are:\n",
    "\n",
    "* multiple linear regression\n",
    "\n",
    "* support vector regression\n",
    "\n",
    "* decision tree regression\n",
    "\n",
    "* random forest regression\n",
    "\n",
    "* gradient boosting regression\n",
    "\n",
    "\n",
    "### Feature selection and engineering\n",
    "\n",
    "\n",
    "* companyId is not associated with salary under any of the encoding schemes we tried, hence we could try dropping it from the data.\n",
    "\n",
    "* Target encoding would be a good encoding system to try as it doesn't produce a high number of features (and so doesn't require us to drop companyId) and gives good correlations with salary.\n",
    "\n",
    "* For linear models, we need to scale our features.\n",
    "\n",
    "* We could engineer pairwise interaction features. These may help with linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 3 - DEVELOP</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Engineer features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # n_estimators=100\n",
    "# # rf_regression = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "\n",
    "\n",
    "dummy_encoder = encoders.Dummy_Encoder(exclude=[\"companyId\"])\n",
    "target_encoder = encoders.Target_Encoder(np.mean, \"salary\")\n",
    "ordinal_encoder = encoders.Ordinal_Encoder(np.mean, \"salary\")\n",
    "\n",
    "encoders_dict = {\n",
    "    \"Dummy coding\": dummy_encoder,\n",
    "    \"Target encoding\": target_encoder,\n",
    "    \"Ordinal encoding\": ordinal_encoder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   25.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression with Dummy coding\n",
      "384.414337322381\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   18.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 5 with Dummy coding\n",
      "721.8098598289428\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   22.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 10 with Dummy coding\n",
      "489.9384748614622\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   24.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 15 with Dummy coding\n",
      "411.4457149937386\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   26.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 20 with Dummy coding\n",
      "471.0236813955738\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   31.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 25 with Dummy coding\n",
      "615.4045526816446\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   10.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 5 with Target encoding\n",
      "585.8566328955716\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 10 with Target encoding\n",
      "408.29033448244184\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 15 with Target encoding\n",
      "414.5021317989128\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 20 with Target encoding\n",
      "621.4393915708008\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   37.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 25 with Target encoding\n",
      "736.9349045721699\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled SVR with Target encoding\n",
      "400.9874961024792\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   16.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 5 with Ordinal encoding\n",
      "585.8566328955716\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 10 with Ordinal encoding\n",
      "408.29033448244184\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   23.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 15 with Ordinal encoding\n",
      "414.422795151447\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   27.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 20 with Ordinal encoding\n",
      "620.6791360991081\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR maxdepth 25 with Ordinal encoding\n",
      "736.5237890196073\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   42.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled SVR with Ordinal encoding\n",
      "398.26120060300957\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dummy_regressors_dict = {}\n",
    "group_regressors_dict = {}\n",
    "\n",
    "which_dict = {\n",
    "    dummy_encoder: dummy_regressors_dict,\n",
    "    target_encoder: group_regressors_dict,\n",
    "    ordinal_encoder: group_regressors_dict,\n",
    "}\n",
    "\n",
    "\n",
    "dummy_regressors_dict[\"Linear regression\"] = LinearRegression()\n",
    "\n",
    "for md in [5, 10, 15, 20, 25]:\n",
    "    group_regressors_dict['DTR maxdepth {}'.format(md)] = DecisionTreeRegressor(max_depth=md)\n",
    "    dummy_regressors_dict['DTR maxdepth {}'.format(md)] = DecisionTreeRegressor(max_depth=md)\n",
    "\n",
    "sv_regression = LinearSVR(verbose=True)\n",
    "scaled_sv_regression = make_pipeline(StandardScaler(), sv_regression, verbose=True)\n",
    "group_regressors_dict[\"Scaled SVR\"] = scaled_sv_regression\n",
    "\n",
    "\n",
    "def cv_mse(estimator, X, y):\n",
    "    return cross_val_score(\n",
    "        estimator=estimator,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        n_jobs=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "for enc_name, enc_obj in encoders_dict.items():\n",
    "\n",
    "    X_train, y_train, encoded_test = preprocessing.encode_and_split(\n",
    "        enc_obj, train_data, test_data, target=\"salary\"\n",
    "    )\n",
    "\n",
    "    for regressor_name, regressor in which_dict[enc_obj].items():\n",
    "        cvs = cv_mse(regressor, X_train, y_train)\n",
    "        print(\n",
    "            \"{} with {}\\n{}\\n\\n\".format(regressor_name, enc_name, (cvs * -1).mean())\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Create models -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Test models -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Select best model -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Part 4 - DEPLOY</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Automate pipeline -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Deploy solution -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---- Measure efficacy -----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
